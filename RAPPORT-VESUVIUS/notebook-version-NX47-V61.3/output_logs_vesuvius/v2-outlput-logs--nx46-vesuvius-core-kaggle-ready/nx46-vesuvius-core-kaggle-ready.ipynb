{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8046770b",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-02-16T16:06:21.228639Z",
     "iopub.status.busy": "2026-02-16T16:06:21.228243Z",
     "iopub.status.idle": "2026-02-16T16:06:21.738878Z",
     "shell.execute_reply": "2026-02-16T16:06:21.737793Z"
    },
    "papermill": {
     "duration": 0.51813,
     "end_time": "2026-02-16T16:06:21.741616",
     "exception": false,
     "start_time": "2026-02-16T16:06:21.223486",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ROADMAP] audit_discovery: 100.0%\n",
      "[ROADMAP] package_submission: 30.0%\n",
      "[ROADMAP] package_submission: 60.0%\n",
      "[ROADMAP] package_submission: 100.0%\n",
      "[ROADMAP] finalize_forensics: 60.0%\n",
      "[ROADMAP] finalize_forensics: 100.0%\n",
      "{\n",
      "  \"status\": \"100%_OFFLINE_ACTIVATED\",\n",
      "  \"active_neurons\": 0,\n",
      "  \"total_allocations\": 0,\n",
      "  \"total_pixels_processed\": 0,\n",
      "  \"total_ink_pixels\": 0,\n",
      "  \"ink_ratio\": 0.0,\n",
      "  \"qi_index_real\": 0.0,\n",
      "  \"cpu_total_ns\": 1272266,\n",
      "  \"merkle_root\": null,\n",
      "  \"layout_detected\": \"empty\",\n",
      "  \"train_items\": [],\n",
      "  \"test_items\": [],\n",
      "  \"train_threshold\": 0.5,\n",
      "  \"submission_csv\": null,\n",
      "  \"submission_zip\": null,\n",
      "  \"zip_members_validated\": null,\n",
      "  \"zip_missing\": [],\n",
      "  \"zip_extra\": [],\n",
      "  \"roadmap_percent\": {\n",
      "    \"audit_discovery\": 100.0,\n",
      "    \"train_thresholds\": 0.0,\n",
      "    \"infer_predictions\": 0.0,\n",
      "    \"package_submission\": 100.0,\n",
      "    \"finalize_forensics\": 60.0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"NX-46 Vesuvius core (Kaggle-ready v2, offline-first).\n",
    "\n",
    "Objectifs v2:\n",
    "- Corriger la non-détection de données (train/test fragments VS train_images/test_images).\n",
    "- Garder un pipeline 100% offline exécutable dans Kaggle.\n",
    "- Produire les artefacts de logs attendus + état final explicite.\n",
    "- Générer submission.csv (format tabulaire) ET submission.zip (format TIFF par id) selon les entrées.\n",
    "- Valider strictement la conformité des membres du zip aux fichiers test attendus.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "from dataclasses import dataclass\n",
    "from hashlib import sha512\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Sequence, Tuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    import tifffile  # type: ignore\n",
    "except Exception:  # pragma: no cover - fallback en environnement réduit\n",
    "    tifffile = None\n",
    "\n",
    "try:\n",
    "    import imageio.v3 as iio\n",
    "except Exception:  # pragma: no cover - fallback en environnement réduit\n",
    "    iio = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class NX46Config:\n",
    "    data_root: str = \"/kaggle/input/vesuvius-challenge-ink-detection\"\n",
    "    work_root: str = \"/kaggle/working/nx46_vesuvius\"\n",
    "    seed: int = 46\n",
    "    bit_capture_bytes: int = 256\n",
    "    threshold_quantile: float = 0.985\n",
    "    slab_min_neurons: int = 128\n",
    "\n",
    "\n",
    "class ProgressRoadmap:\n",
    "    def __init__(self) -> None:\n",
    "        self.steps: Dict[str, float] = {\n",
    "            \"audit_discovery\": 0.0,\n",
    "            \"train_thresholds\": 0.0,\n",
    "            \"infer_predictions\": 0.0,\n",
    "            \"package_submission\": 0.0,\n",
    "            \"finalize_forensics\": 0.0,\n",
    "        }\n",
    "\n",
    "    def update(self, step: str, pct: float) -> None:\n",
    "        self.steps[step] = max(0.0, min(100.0, float(pct)))\n",
    "        print(f\"[ROADMAP] {step}: {self.steps[step]:.1f}%\")\n",
    "\n",
    "    def as_dict(self) -> Dict[str, float]:\n",
    "        return dict(self.steps)\n",
    "\n",
    "\n",
    "class HFBL360Logger:\n",
    "    def __init__(self, root: Path) -> None:\n",
    "        self.root = root\n",
    "        self.root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        self.log_path = root / \"forensic_ultra.log\"\n",
    "        self.core_log = root / \"nx-46-vesuvius-core.log\"\n",
    "        self.kaggle_log = root / \"nx46-vesuvius-core-kaggle-ready.log\"\n",
    "        self.csv_path = root / \"metrics.csv\"\n",
    "        self.state_path = root / \"state.json\"\n",
    "        self.bit_path = root / \"bit_capture.log\"\n",
    "        self.merkle_path = root / \"merkle_chain.log\"\n",
    "\n",
    "        with self.csv_path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            csv.writer(f).writerow(\n",
    "                [\n",
    "                    \"timestamp_ns\",\n",
    "                    \"phase\",\n",
    "                    \"fragment\",\n",
    "                    \"neurons_active\",\n",
    "                    \"cpu_ns\",\n",
    "                    \"ink_pixels\",\n",
    "                    \"total_pixels\",\n",
    "                    \"ink_ratio\",\n",
    "                    \"merkle_prefix\",\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    def _append_all_logs(self, text: str) -> None:\n",
    "        for p in (self.log_path, self.core_log, self.kaggle_log):\n",
    "            with p.open(\"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(text + \"\\n\")\n",
    "\n",
    "    def log_event(self, message: str) -> None:\n",
    "        self._append_all_logs(f\"{time.time_ns()} | {message}\")\n",
    "\n",
    "    def log_bits(self, fragment: str, payload: bytes) -> None:\n",
    "        bit_string = \"\".join(f\"{b:08b}\" for b in payload)\n",
    "        with self.bit_path.open(\"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"{time.time_ns()} | {fragment} | {bit_string}\\n\")\n",
    "\n",
    "    def log_merkle(self, fragment: str, digest: str) -> None:\n",
    "        with self.merkle_path.open(\"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"{time.time_ns()} | {fragment} | {digest}\\n\")\n",
    "\n",
    "    def log_metrics(\n",
    "        self,\n",
    "        *,\n",
    "        phase: str,\n",
    "        fragment: str,\n",
    "        neurons_active: int,\n",
    "        cpu_ns: int,\n",
    "        ink_pixels: int,\n",
    "        total_pixels: int,\n",
    "        merkle_prefix: str,\n",
    "    ) -> None:\n",
    "        ratio = (ink_pixels / total_pixels) if total_pixels else 0.0\n",
    "        with self.csv_path.open(\"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            csv.writer(f).writerow(\n",
    "                [\n",
    "                    time.time_ns(),\n",
    "                    phase,\n",
    "                    fragment,\n",
    "                    neurons_active,\n",
    "                    cpu_ns,\n",
    "                    ink_pixels,\n",
    "                    total_pixels,\n",
    "                    f\"{ratio:.8f}\",\n",
    "                    merkle_prefix,\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    def write_state(self, state: Dict[str, object]) -> None:\n",
    "        state = dict(state)\n",
    "        state[\"timestamp_ns\"] = time.time_ns()\n",
    "        with self.state_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(state, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "\n",
    "class NX46AGNNVesuvius:\n",
    "    def __init__(self, cfg: NX46Config) -> None:\n",
    "        self.cfg = cfg\n",
    "        self.rng = np.random.default_rng(cfg.seed)\n",
    "        self.logger = HFBL360Logger(Path(cfg.work_root) / \"logs\")\n",
    "        self.neurons_active = 0\n",
    "        self.total_allocations = 0\n",
    "        self.total_pixels_processed = 0\n",
    "        self.total_ink_pixels = 0\n",
    "        self.merkle_chain: List[str] = []\n",
    "        self.global_cpu_start_ns = time.process_time_ns()\n",
    "        self.logger.log_event(\"SYSTEM_STARTUP_L0_SUCCESS\")\n",
    "\n",
    "    def slab_allocate(self, tensor: np.ndarray, phase: str) -> int:\n",
    "        variance = float(np.var(tensor, dtype=np.float64))\n",
    "        entropy_proxy = float(np.mean(np.abs(np.gradient(tensor.astype(np.float32), axis=-1))))\n",
    "        required = int(\n",
    "            self.cfg.slab_min_neurons\n",
    "            + (tensor.size // 512)\n",
    "            + variance * 1500.0\n",
    "            + entropy_proxy * 900.0\n",
    "        )\n",
    "        self.neurons_active = max(self.cfg.slab_min_neurons, required)\n",
    "        self.total_allocations += 1\n",
    "        self.logger.log_event(\n",
    "            f\"SLAB_ALLOCATION phase={phase} neurons={self.neurons_active} variance={variance:.8f} entropy_proxy={entropy_proxy:.8f}\"\n",
    "        )\n",
    "        return self.neurons_active\n",
    "\n",
    "    def _track_bits(self, fragment: str, arr: np.ndarray) -> None:\n",
    "        self.logger.log_bits(fragment, arr.tobytes()[: self.cfg.bit_capture_bytes])\n",
    "\n",
    "    def _merkle_sign(self, fragment: str, arr: np.ndarray) -> str:\n",
    "        prev = self.merkle_chain[-1] if self.merkle_chain else \"GENESIS\"\n",
    "        digest = sha512(prev.encode(\"utf-8\") + arr.tobytes()).hexdigest()\n",
    "        self.merkle_chain.append(digest)\n",
    "        self.logger.log_merkle(fragment, digest)\n",
    "        return digest\n",
    "\n",
    "    @staticmethod\n",
    "    def _normalize_stack(stack: np.ndarray) -> np.ndarray:\n",
    "        x = stack.astype(np.float32)\n",
    "        mn, mx = float(x.min()), float(x.max())\n",
    "        if mx <= mn:\n",
    "            return np.zeros_like(x, dtype=np.float32)\n",
    "        return (x - mn) / (mx - mn)\n",
    "\n",
    "    @staticmethod\n",
    "    def _ink_energy_projection(stack: np.ndarray) -> np.ndarray:\n",
    "        grad_z = np.abs(np.diff(stack, axis=0, prepend=stack[:1]))\n",
    "        grad_y = np.abs(np.diff(stack, axis=1, prepend=stack[:, :1, :]))\n",
    "        grad_x = np.abs(np.diff(stack, axis=2, prepend=stack[:, :, :1]))\n",
    "        return np.mean(0.45 * grad_z + 0.30 * grad_y + 0.25 * grad_x, axis=0)\n",
    "\n",
    "    def train_threshold(self, stack: np.ndarray, labels: np.ndarray, fragment: str) -> float:\n",
    "        start = time.process_time_ns()\n",
    "        self.slab_allocate(stack, phase=\"train\")\n",
    "        self._track_bits(fragment, stack)\n",
    "\n",
    "        score = self._ink_energy_projection(self._normalize_stack(stack))\n",
    "        pos = score[labels > 0]\n",
    "        neg = score[labels <= 0]\n",
    "\n",
    "        if pos.size and neg.size:\n",
    "            threshold = float(0.5 * (float(np.median(pos)) + float(np.median(neg))))\n",
    "        elif pos.size:\n",
    "            threshold = float(np.quantile(pos, 0.50))\n",
    "        else:\n",
    "            threshold = float(np.quantile(score, self.cfg.threshold_quantile))\n",
    "\n",
    "        pred = (score >= threshold).astype(np.uint8)\n",
    "        digest = self._merkle_sign(fragment, score)\n",
    "        cpu_ns = time.process_time_ns() - start\n",
    "\n",
    "        ink_pixels = int(pred.sum())\n",
    "        total_pixels = int(pred.size)\n",
    "        self.total_ink_pixels += ink_pixels\n",
    "        self.total_pixels_processed += total_pixels\n",
    "\n",
    "        self.logger.log_metrics(\n",
    "            phase=\"train\",\n",
    "            fragment=fragment,\n",
    "            neurons_active=self.neurons_active,\n",
    "            cpu_ns=cpu_ns,\n",
    "            ink_pixels=ink_pixels,\n",
    "            total_pixels=total_pixels,\n",
    "            merkle_prefix=digest[:16],\n",
    "        )\n",
    "        self.logger.log_event(f\"TRAIN_DONE fragment={fragment} threshold={threshold:.8f}\")\n",
    "        return threshold\n",
    "\n",
    "    def infer_mask(self, stack: np.ndarray, threshold: float, fragment: str) -> np.ndarray:\n",
    "        start = time.process_time_ns()\n",
    "        self.slab_allocate(stack, phase=\"infer\")\n",
    "        self._track_bits(fragment, stack)\n",
    "\n",
    "        score = self._ink_energy_projection(self._normalize_stack(stack))\n",
    "        pred = (score >= threshold).astype(np.uint8)\n",
    "        digest = self._merkle_sign(fragment, pred)\n",
    "        cpu_ns = time.process_time_ns() - start\n",
    "\n",
    "        ink_pixels = int(pred.sum())\n",
    "        total_pixels = int(pred.size)\n",
    "        self.total_ink_pixels += ink_pixels\n",
    "        self.total_pixels_processed += total_pixels\n",
    "\n",
    "        self.logger.log_metrics(\n",
    "            phase=\"infer\",\n",
    "            fragment=fragment,\n",
    "            neurons_active=self.neurons_active,\n",
    "            cpu_ns=cpu_ns,\n",
    "            ink_pixels=ink_pixels,\n",
    "            total_pixels=total_pixels,\n",
    "            merkle_prefix=digest[:16],\n",
    "        )\n",
    "        return pred\n",
    "\n",
    "    def finalize(self, extra: Optional[Dict[str, object]] = None) -> Dict[str, object]:\n",
    "        cpu_total_ns = time.process_time_ns() - self.global_cpu_start_ns\n",
    "        qi_index = self.total_pixels_processed / max(cpu_total_ns, 1)\n",
    "        state = {\n",
    "            \"status\": \"100%_OFFLINE_ACTIVATED\",\n",
    "            \"active_neurons\": self.neurons_active,\n",
    "            \"total_allocations\": self.total_allocations,\n",
    "            \"total_pixels_processed\": self.total_pixels_processed,\n",
    "            \"total_ink_pixels\": self.total_ink_pixels,\n",
    "            \"ink_ratio\": self.total_ink_pixels / self.total_pixels_processed if self.total_pixels_processed else 0.0,\n",
    "            \"qi_index_real\": qi_index,\n",
    "            \"cpu_total_ns\": cpu_total_ns,\n",
    "            \"merkle_root\": self.merkle_chain[-1] if self.merkle_chain else None,\n",
    "        }\n",
    "        if extra:\n",
    "            state.update(extra)\n",
    "        self.logger.write_state(state)\n",
    "        self.logger.log_event(\"SYSTEM_LOADED_100_PERCENT\")\n",
    "        return state\n",
    "\n",
    "\n",
    "def _read_tif(path: Path) -> np.ndarray:\n",
    "    if tifffile is not None:\n",
    "        arr = tifffile.imread(path)\n",
    "        return np.asarray(arr)\n",
    "    if iio is None:\n",
    "        raise RuntimeError(\"tifffile et imageio indisponibles: impossible de lire les TIFF.\")\n",
    "    return np.asarray(iio.imread(str(path)))\n",
    "\n",
    "\n",
    "def _write_tif(path: Path, arr2d: np.ndarray) -> None:\n",
    "    arr2d = (arr2d > 0).astype(np.uint8) * 255\n",
    "    if tifffile is not None:\n",
    "        tifffile.imwrite(path, arr2d, compression=\"LZW\")\n",
    "        return\n",
    "    if iio is None:\n",
    "        raise RuntimeError(\"tifffile et imageio indisponibles: impossible d'écrire les TIFF.\")\n",
    "    iio.imwrite(str(path), arr2d)\n",
    "\n",
    "\n",
    "def _to_stack(raw: np.ndarray) -> np.ndarray:\n",
    "    a = np.asarray(raw)\n",
    "    if a.ndim == 2:\n",
    "        return a[None, :, :]\n",
    "    if a.ndim == 3:\n",
    "        return a\n",
    "    raise ValueError(f\"Unsupported TIFF ndim={a.ndim}; attendu 2D ou 3D.\")\n",
    "\n",
    "\n",
    "def _read_stack_from_fragment(fragment_dir: Path) -> np.ndarray:\n",
    "    volume_dir = fragment_dir / \"surface_volume\"\n",
    "    tif_files = sorted(volume_dir.glob(\"*.tif\"))\n",
    "    if not tif_files:\n",
    "        raise FileNotFoundError(f\"No TIFF slices found in {volume_dir}\")\n",
    "    slices = [_to_stack(_read_tif(p))[0] for p in tif_files]\n",
    "    return np.stack(slices, axis=0)\n",
    "\n",
    "\n",
    "def _load_label_png(fragment_dir: Path) -> Optional[np.ndarray]:\n",
    "    p = fragment_dir / \"inklabels.png\"\n",
    "    if not p.exists() or iio is None:\n",
    "        return None\n",
    "    arr = np.asarray(iio.imread(str(p)))\n",
    "    if arr.ndim == 3:\n",
    "        arr = arr[..., 0]\n",
    "    return (arr > 0).astype(np.uint8)\n",
    "\n",
    "\n",
    "def _read_sample_submission_ids(sample_csv: Path) -> List[str]:\n",
    "    ids: List[str] = []\n",
    "    with sample_csv.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        cols = set(reader.fieldnames or [])\n",
    "        id_col = \"Id\" if \"Id\" in cols else (\"id\" if \"id\" in cols else None)\n",
    "        if id_col is None:\n",
    "            return ids\n",
    "        for row in reader:\n",
    "            v = str(row[id_col]).strip()\n",
    "            if v:\n",
    "                ids.append(v)\n",
    "    return ids\n",
    "\n",
    "\n",
    "def _discover_layout(root: Path) -> Tuple[str, List[Path], List[Path], List[Path]]:\n",
    "    train_fragment_dirs = sorted([p for p in (root / \"train\").glob(\"*\") if p.is_dir()]) if (root / \"train\").exists() else []\n",
    "    test_fragment_dirs = sorted([p for p in (root / \"test\").glob(\"*\") if p.is_dir()]) if (root / \"test\").exists() else []\n",
    "    train_image_tifs = sorted((root / \"train_images\").glob(\"*.tif\")) if (root / \"train_images\").exists() else []\n",
    "    test_image_tifs = sorted((root / \"test_images\").glob(\"*.tif\")) if (root / \"test_images\").exists() else []\n",
    "\n",
    "    if train_fragment_dirs or test_fragment_dirs:\n",
    "        return \"fragments\", train_fragment_dirs, test_fragment_dirs, []\n",
    "    if train_image_tifs or test_image_tifs:\n",
    "        return \"images\", train_image_tifs, test_image_tifs, []\n",
    "    return \"empty\", [], [], []\n",
    "\n",
    "\n",
    "def _safe_resize_pair(stack: np.ndarray, labels: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    h = min(stack.shape[1], labels.shape[0])\n",
    "    w = min(stack.shape[2], labels.shape[1])\n",
    "    return stack[:, :h, :w], labels[:h, :w]\n",
    "\n",
    "\n",
    "def _write_submission_csv(out_path: Path, sample_csv: Path, predictions_by_id: Dict[str, np.ndarray]) -> Optional[str]:\n",
    "    ids = _read_sample_submission_ids(sample_csv)\n",
    "    if not ids:\n",
    "        return None\n",
    "\n",
    "    flat = np.concatenate([predictions_by_id[i].reshape(-1).astype(np.uint8) for i in sorted(predictions_by_id)])\n",
    "    n = min(len(ids), len(flat))\n",
    "\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with out_path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"Id\", \"Predicted\"])\n",
    "        for i in range(n):\n",
    "            writer.writerow([ids[i], int(flat[i])])\n",
    "    return str(out_path)\n",
    "\n",
    "\n",
    "def _write_submission_zip(\n",
    "    zip_path: Path,\n",
    "    masks_dir: Path,\n",
    "    predictions_by_id: Dict[str, np.ndarray],\n",
    "    expected_ids: Sequence[str],\n",
    ") -> Tuple[str, bool, List[str], List[str]]:\n",
    "    masks_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    written_files: List[str] = []\n",
    "    for fid, mask in predictions_by_id.items():\n",
    "        out_tif = masks_dir / f\"{fid}.tif\"\n",
    "        _write_tif(out_tif, mask)\n",
    "        written_files.append(out_tif.name)\n",
    "\n",
    "    with zipfile.ZipFile(zip_path, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "        for name in sorted(written_files):\n",
    "            zf.write(masks_dir / name, arcname=name)\n",
    "\n",
    "    got = sorted(written_files)\n",
    "    exp = sorted([f\"{x}.tif\" for x in expected_ids]) if expected_ids else got\n",
    "    ok = got == exp\n",
    "    missing = sorted(set(exp) - set(got))\n",
    "    extra = sorted(set(got) - set(exp))\n",
    "    return str(zip_path), ok, missing, extra\n",
    "\n",
    "\n",
    "def _emit_execution_txts(log_root: Path, payload: Dict[str, object]) -> None:\n",
    "    content = json.dumps(payload, indent=2, ensure_ascii=False)\n",
    "    for name in (\"RkF4XakI.txt\", \"UJxLRsEE.txt\"):\n",
    "        with (log_root / name).open(\"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(content + \"\\n\")\n",
    "\n",
    "\n",
    "def run_pipeline(cfg: NX46Config) -> Dict[str, object]:\n",
    "    root = Path(cfg.data_root)\n",
    "    work = Path(cfg.work_root)\n",
    "    logs_root = work / \"logs\"\n",
    "    logs_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    roadmap = ProgressRoadmap()\n",
    "    nx46 = NX46AGNNVesuvius(cfg)\n",
    "\n",
    "    layout, train_items, test_items, _ = _discover_layout(root)\n",
    "    roadmap.update(\"audit_discovery\", 100.0)\n",
    "    nx46.logger.log_event(f\"LAYOUT_DETECTED={layout} train_items={len(train_items)} test_items={len(test_items)}\")\n",
    "\n",
    "    thresholds: List[float] = []\n",
    "\n",
    "    if layout == \"fragments\":\n",
    "        total_train = max(len(train_items), 1)\n",
    "        for idx, frag in enumerate(train_items, start=1):\n",
    "            pct = idx / total_train * 100.0\n",
    "            roadmap.update(\"train_thresholds\", pct)\n",
    "            nx46.logger.log_event(f\"PROGRESS train={idx}/{len(train_items)} ({pct:.1f}%)\")\n",
    "\n",
    "            volume_dir = frag / \"surface_volume\"\n",
    "            if not volume_dir.exists():\n",
    "                continue\n",
    "            stack = _read_stack_from_fragment(frag)\n",
    "            labels = _load_label_png(frag)\n",
    "            if labels is None:\n",
    "                continue\n",
    "            if labels.shape != stack.shape[1:]:\n",
    "                stack, labels = _safe_resize_pair(stack, labels)\n",
    "            thresholds.append(nx46.train_threshold(stack, labels, frag.name))\n",
    "\n",
    "    threshold = float(np.median(np.asarray(thresholds, dtype=np.float32))) if thresholds else 0.5\n",
    "\n",
    "    predictions: Dict[str, np.ndarray] = {}\n",
    "\n",
    "    if layout == \"fragments\":\n",
    "        total_test = max(len(test_items), 1)\n",
    "        for idx, frag in enumerate(test_items, start=1):\n",
    "            pct = idx / total_test * 100.0\n",
    "            roadmap.update(\"infer_predictions\", pct)\n",
    "            nx46.logger.log_event(f\"PROGRESS test={idx}/{len(test_items)} ({pct:.1f}%)\")\n",
    "            volume_dir = frag / \"surface_volume\"\n",
    "            if not volume_dir.exists():\n",
    "                continue\n",
    "            stack = _read_stack_from_fragment(frag)\n",
    "            predictions[frag.name] = nx46.infer_mask(stack, threshold, frag.name)\n",
    "\n",
    "    elif layout == \"images\":\n",
    "        total_test = max(len(test_items), 1)\n",
    "        for idx, tif_path in enumerate(test_items, start=1):\n",
    "            pct = idx / total_test * 100.0\n",
    "            roadmap.update(\"infer_predictions\", pct)\n",
    "            nx46.logger.log_event(f\"PROGRESS test_images={idx}/{len(test_items)} ({pct:.1f}%)\")\n",
    "            stack = _to_stack(_read_tif(tif_path))\n",
    "            predictions[tif_path.stem] = nx46.infer_mask(stack, threshold, tif_path.stem)\n",
    "\n",
    "    roadmap.update(\"package_submission\", 30.0)\n",
    "\n",
    "    sample_csv_candidates = [root / \"sample_submission.csv\", Path(\"/kaggle/input/vesuvius-challenge-ink-detection/sample_submission.csv\")]\n",
    "    sample_csv = next((p for p in sample_csv_candidates if p.exists()), None)\n",
    "\n",
    "    csv_submission_path: Optional[str] = None\n",
    "    expected_ids: List[str] = []\n",
    "    if sample_csv is not None:\n",
    "        expected_ids = _read_sample_submission_ids(sample_csv)\n",
    "        if predictions:\n",
    "            csv_submission_path = _write_submission_csv(work / \"submission.csv\", sample_csv, predictions)\n",
    "\n",
    "    roadmap.update(\"package_submission\", 60.0)\n",
    "\n",
    "    zip_submission_path: Optional[str] = None\n",
    "    zip_ok: Optional[bool] = None\n",
    "    zip_missing: List[str] = []\n",
    "    zip_extra: List[str] = []\n",
    "    if predictions:\n",
    "        zip_submission_path, zip_ok, zip_missing, zip_extra = _write_submission_zip(\n",
    "            work / \"submission.zip\",\n",
    "            work / \"submission_masks\",\n",
    "            predictions,\n",
    "            expected_ids if expected_ids else sorted(predictions.keys()),\n",
    "        )\n",
    "\n",
    "    roadmap.update(\"package_submission\", 100.0)\n",
    "    roadmap.update(\"finalize_forensics\", 60.0)\n",
    "\n",
    "    result = nx46.finalize(\n",
    "        {\n",
    "            \"layout_detected\": layout,\n",
    "            \"train_items\": [p.name for p in train_items],\n",
    "            \"test_items\": [p.name for p in test_items],\n",
    "            \"train_threshold\": threshold,\n",
    "            \"submission_csv\": csv_submission_path,\n",
    "            \"submission_zip\": zip_submission_path,\n",
    "            \"zip_members_validated\": zip_ok,\n",
    "            \"zip_missing\": zip_missing,\n",
    "            \"zip_extra\": zip_extra,\n",
    "            \"roadmap_percent\": roadmap.as_dict(),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    roadmap.update(\"finalize_forensics\", 100.0)\n",
    "    _emit_execution_txts(logs_root, result)\n",
    "    return result\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = NX46Config(\n",
    "        data_root=os.environ.get(\"NX46_DATA_ROOT\", NX46Config.data_root),\n",
    "        work_root=os.environ.get(\"NX46_WORK_ROOT\", NX46Config.work_root),\n",
    "    )\n",
    "    final_state = run_pipeline(config)\n",
    "    print(json.dumps(final_state, indent=2, ensure_ascii=False))\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 15062069,
     "sourceId": 117682,
     "sourceType": "competition"
    },
    {
     "datasetId": 9431333,
     "sourceId": 14755914,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9462392,
     "sourceId": 14799025,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9470090,
     "sourceId": 14809675,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4.513136,
   "end_time": "2026-02-16T16:06:22.163341",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-02-16T16:06:17.650205",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
