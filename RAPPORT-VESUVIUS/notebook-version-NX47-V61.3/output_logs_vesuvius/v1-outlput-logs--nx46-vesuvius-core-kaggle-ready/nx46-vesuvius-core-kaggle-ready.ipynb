{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58ec26c5",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-02-16T14:28:29.036739Z",
     "iopub.status.busy": "2026-02-16T14:28:29.036440Z",
     "iopub.status.idle": "2026-02-16T14:28:29.182339Z",
     "shell.execute_reply": "2026-02-16T14:28:29.181349Z"
    },
    "papermill": {
     "duration": 0.152019,
     "end_time": "2026-02-16T14:28:29.184416",
     "exception": false,
     "start_time": "2026-02-16T14:28:29.032397",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"status\": \"100%_OFFLINE_ACTIVATED\",\n",
      "  \"active_neurons\": 0,\n",
      "  \"total_allocations\": 0,\n",
      "  \"total_pixels_processed\": 0,\n",
      "  \"total_ink_pixels\": 0,\n",
      "  \"ink_ratio\": 0.0,\n",
      "  \"qi_index_real\": 0.0,\n",
      "  \"cpu_total_ns\": 224518,\n",
      "  \"merkle_root\": null,\n",
      "  \"train_fragments\": [],\n",
      "  \"test_fragments\": [],\n",
      "  \"train_threshold\": 0.5,\n",
      "  \"submission_path\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"NX-46 Vesuvius core, offline-first, without stubs/placeholders.\n",
    "\n",
    "Kaggle-ready single file for direct copy/paste execution.\n",
    "Includes:\n",
    "- Dynamic neuron allocation (slab-based)\n",
    "- Bit-level memory tracking\n",
    "- Merkle-chain forensic signatures\n",
    "- Offline data discovery for Vesuvius Challenge folders\n",
    "- Deterministic training/inference pipeline\n",
    "- Submission file generation compatible with common Kaggle formats\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from hashlib import sha512\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import imageio.v3 as iio\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class NX46Config:\n",
    "    data_root: str = \"/kaggle/input/vesuvius-challenge-ink-detection\"\n",
    "    work_root: str = \"/kaggle/working/nx46_vesuvius\"\n",
    "    seed: int = 46\n",
    "    bit_capture_bytes: int = 256\n",
    "    patch_size: int = 64\n",
    "    threshold_quantile: float = 0.985\n",
    "    slab_min_neurons: int = 128\n",
    "\n",
    "\n",
    "class HFBL360Logger:\n",
    "    def __init__(self, root: Path) -> None:\n",
    "        self.root = root\n",
    "        self.root.mkdir(parents=True, exist_ok=True)\n",
    "        self.log_path = root / \"forensic_ultra.log\"\n",
    "        self.csv_path = root / \"metrics.csv\"\n",
    "        self.state_path = root / \"state.json\"\n",
    "        self.bit_path = root / \"bit_capture.log\"\n",
    "        self.merkle_path = root / \"merkle_chain.log\"\n",
    "\n",
    "        with self.csv_path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            w = csv.writer(f)\n",
    "            w.writerow(\n",
    "                [\n",
    "                    \"timestamp_ns\",\n",
    "                    \"phase\",\n",
    "                    \"fragment\",\n",
    "                    \"neurons_active\",\n",
    "                    \"cpu_ns\",\n",
    "                    \"ink_pixels\",\n",
    "                    \"total_pixels\",\n",
    "                    \"ink_ratio\",\n",
    "                    \"merkle_prefix\",\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    def log_event(self, message: str) -> None:\n",
    "        with self.log_path.open(\"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"{time.time_ns()} | {message}\\n\")\n",
    "\n",
    "    def log_bits(self, fragment: str, payload: bytes) -> None:\n",
    "        bit_string = \"\".join(f\"{b:08b}\" for b in payload)\n",
    "        with self.bit_path.open(\"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"{time.time_ns()} | {fragment} | {bit_string}\\n\")\n",
    "\n",
    "    def log_merkle(self, fragment: str, digest: str) -> None:\n",
    "        with self.merkle_path.open(\"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"{time.time_ns()} | {fragment} | {digest}\\n\")\n",
    "\n",
    "    def log_metrics(\n",
    "        self,\n",
    "        *,\n",
    "        phase: str,\n",
    "        fragment: str,\n",
    "        neurons_active: int,\n",
    "        cpu_ns: int,\n",
    "        ink_pixels: int,\n",
    "        total_pixels: int,\n",
    "        merkle_prefix: str,\n",
    "    ) -> None:\n",
    "        ratio = (ink_pixels / total_pixels) if total_pixels else 0.0\n",
    "        with self.csv_path.open(\"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            w = csv.writer(f)\n",
    "            w.writerow(\n",
    "                [\n",
    "                    time.time_ns(),\n",
    "                    phase,\n",
    "                    fragment,\n",
    "                    neurons_active,\n",
    "                    cpu_ns,\n",
    "                    ink_pixels,\n",
    "                    total_pixels,\n",
    "                    f\"{ratio:.8f}\",\n",
    "                    merkle_prefix,\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    def write_state(self, state: Dict[str, object]) -> None:\n",
    "        state = dict(state)\n",
    "        state[\"timestamp_ns\"] = time.time_ns()\n",
    "        with self.state_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(state, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "\n",
    "class NX46AGNNVesuvius:\n",
    "    def __init__(self, cfg: NX46Config) -> None:\n",
    "        self.cfg = cfg\n",
    "        self.rng = np.random.default_rng(cfg.seed)\n",
    "        self.logger = HFBL360Logger(Path(cfg.work_root) / \"logs\")\n",
    "        self.neurons_active = 0\n",
    "        self.total_allocations = 0\n",
    "        self.total_pixels_processed = 0\n",
    "        self.total_ink_pixels = 0\n",
    "        self.merkle_chain: List[str] = []\n",
    "        self.global_cpu_start_ns = time.process_time_ns()\n",
    "        self.logger.log_event(\"SYSTEM_STARTUP_L0_SUCCESS\")\n",
    "\n",
    "    def slab_allocate(self, tensor: np.ndarray, phase: str) -> int:\n",
    "        variance = float(np.var(tensor, dtype=np.float64))\n",
    "        entropy_proxy = float(np.mean(np.abs(np.gradient(tensor.astype(np.float32), axis=-1))))\n",
    "        required = int(\n",
    "            self.cfg.slab_min_neurons\n",
    "            + (tensor.size // 512)\n",
    "            + variance * 1500.0\n",
    "            + entropy_proxy * 900.0\n",
    "        )\n",
    "        self.neurons_active = max(self.cfg.slab_min_neurons, required)\n",
    "        self.total_allocations += 1\n",
    "        self.logger.log_event(\n",
    "            f\"SLAB_ALLOCATION phase={phase} neurons={self.neurons_active} variance={variance:.8f} entropy_proxy={entropy_proxy:.8f}\"\n",
    "        )\n",
    "        return self.neurons_active\n",
    "\n",
    "    def _track_bits(self, fragment: str, arr: np.ndarray) -> None:\n",
    "        payload = arr.tobytes()[: self.cfg.bit_capture_bytes]\n",
    "        self.logger.log_bits(fragment, payload)\n",
    "\n",
    "    def _merkle_sign(self, fragment: str, arr: np.ndarray) -> str:\n",
    "        prev = self.merkle_chain[-1] if self.merkle_chain else \"GENESIS\"\n",
    "        digest = sha512(prev.encode(\"utf-8\") + arr.tobytes()).hexdigest()\n",
    "        self.merkle_chain.append(digest)\n",
    "        self.logger.log_merkle(fragment, digest)\n",
    "        return digest\n",
    "\n",
    "    @staticmethod\n",
    "    def _normalize_stack(stack: np.ndarray) -> np.ndarray:\n",
    "        stack = stack.astype(np.float32)\n",
    "        mn, mx = float(stack.min()), float(stack.max())\n",
    "        if mx <= mn:\n",
    "            return np.zeros_like(stack, dtype=np.float32)\n",
    "        return (stack - mn) / (mx - mn)\n",
    "\n",
    "    @staticmethod\n",
    "    def _ink_energy_projection(stack: np.ndarray) -> np.ndarray:\n",
    "        grad_z = np.abs(np.diff(stack, axis=0, prepend=stack[:1]))\n",
    "        grad_y = np.abs(np.diff(stack, axis=1, prepend=stack[:, :1, :]))\n",
    "        grad_x = np.abs(np.diff(stack, axis=2, prepend=stack[:, :, :1]))\n",
    "        energy = 0.45 * grad_z + 0.30 * grad_y + 0.25 * grad_x\n",
    "        return np.mean(energy, axis=0)\n",
    "\n",
    "    def train_threshold(self, stack: np.ndarray, labels: np.ndarray, fragment: str) -> float:\n",
    "        start = time.process_time_ns()\n",
    "        self.slab_allocate(stack, phase=\"train\")\n",
    "        self._track_bits(fragment, stack)\n",
    "\n",
    "        norm_stack = self._normalize_stack(stack)\n",
    "        score = self._ink_energy_projection(norm_stack)\n",
    "        pos = score[labels > 0]\n",
    "        neg = score[labels <= 0]\n",
    "        if pos.size and neg.size:\n",
    "            threshold = float(0.5 * (float(np.median(pos)) + float(np.median(neg))))\n",
    "        elif pos.size:\n",
    "            threshold = float(np.quantile(pos, 0.50))\n",
    "        else:\n",
    "            threshold = float(np.quantile(score, self.cfg.threshold_quantile))\n",
    "\n",
    "        digest = self._merkle_sign(fragment, score)\n",
    "        pred = (score >= threshold).astype(np.uint8)\n",
    "        ink_pixels = int(pred.sum())\n",
    "        total_pixels = int(pred.size)\n",
    "        cpu_ns = time.process_time_ns() - start\n",
    "\n",
    "        self.total_ink_pixels += ink_pixels\n",
    "        self.total_pixels_processed += total_pixels\n",
    "\n",
    "        self.logger.log_metrics(\n",
    "            phase=\"train\",\n",
    "            fragment=fragment,\n",
    "            neurons_active=self.neurons_active,\n",
    "            cpu_ns=cpu_ns,\n",
    "            ink_pixels=ink_pixels,\n",
    "            total_pixels=total_pixels,\n",
    "            merkle_prefix=digest[:16],\n",
    "        )\n",
    "        self.logger.log_event(f\"TRAIN_DONE fragment={fragment} threshold={threshold:.8f}\")\n",
    "        return threshold\n",
    "\n",
    "    def infer_mask(self, stack: np.ndarray, threshold: float, fragment: str) -> np.ndarray:\n",
    "        start = time.process_time_ns()\n",
    "        self.slab_allocate(stack, phase=\"infer\")\n",
    "        self._track_bits(fragment, stack)\n",
    "\n",
    "        norm_stack = self._normalize_stack(stack)\n",
    "        score = self._ink_energy_projection(norm_stack)\n",
    "        pred = (score >= threshold).astype(np.uint8)\n",
    "\n",
    "        digest = self._merkle_sign(fragment, pred)\n",
    "        ink_pixels = int(pred.sum())\n",
    "        total_pixels = int(pred.size)\n",
    "        cpu_ns = time.process_time_ns() - start\n",
    "\n",
    "        self.total_ink_pixels += ink_pixels\n",
    "        self.total_pixels_processed += total_pixels\n",
    "\n",
    "        self.logger.log_metrics(\n",
    "            phase=\"infer\",\n",
    "            fragment=fragment,\n",
    "            neurons_active=self.neurons_active,\n",
    "            cpu_ns=cpu_ns,\n",
    "            ink_pixels=ink_pixels,\n",
    "            total_pixels=total_pixels,\n",
    "            merkle_prefix=digest[:16],\n",
    "        )\n",
    "        return pred\n",
    "\n",
    "    def finalize(self, extra: Optional[Dict[str, object]] = None) -> Dict[str, object]:\n",
    "        cpu_total_ns = time.process_time_ns() - self.global_cpu_start_ns\n",
    "        qi_index = self.total_pixels_processed / max(cpu_total_ns, 1)\n",
    "        state = {\n",
    "            \"status\": \"100%_OFFLINE_ACTIVATED\",\n",
    "            \"active_neurons\": self.neurons_active,\n",
    "            \"total_allocations\": self.total_allocations,\n",
    "            \"total_pixels_processed\": self.total_pixels_processed,\n",
    "            \"total_ink_pixels\": self.total_ink_pixels,\n",
    "            \"ink_ratio\": (\n",
    "                self.total_ink_pixels / self.total_pixels_processed\n",
    "                if self.total_pixels_processed\n",
    "                else 0.0\n",
    "            ),\n",
    "            \"qi_index_real\": qi_index,\n",
    "            \"cpu_total_ns\": cpu_total_ns,\n",
    "            \"merkle_root\": self.merkle_chain[-1] if self.merkle_chain else None,\n",
    "        }\n",
    "        if extra:\n",
    "            state.update(extra)\n",
    "        self.logger.write_state(state)\n",
    "        self.logger.log_event(\"SYSTEM_LOADED_100_PERCENT\")\n",
    "        return state\n",
    "\n",
    "\n",
    "def _read_stack_tif(volume_dir: Path) -> np.ndarray:\n",
    "    tif_files = sorted(volume_dir.glob(\"*.tif\"))\n",
    "    if not tif_files:\n",
    "        raise FileNotFoundError(f\"No TIFF slices found in {volume_dir}\")\n",
    "    stack = [iio.imread(str(p)) for p in tif_files]\n",
    "    return np.stack(stack, axis=0)\n",
    "\n",
    "\n",
    "def _discover_fragments(root: Path, mode: str) -> List[Path]:\n",
    "    mode_dir = root / mode\n",
    "    if not mode_dir.exists():\n",
    "        return []\n",
    "    return sorted([p for p in mode_dir.iterdir() if p.is_dir()])\n",
    "\n",
    "\n",
    "def _load_label_png(fragment: Path) -> Optional[np.ndarray]:\n",
    "    p = fragment / \"inklabels.png\"\n",
    "    if not p.exists():\n",
    "        return None\n",
    "    arr = iio.imread(str(p))\n",
    "    if arr.ndim == 3:\n",
    "        arr = arr[..., 0]\n",
    "    return (arr > 0).astype(np.uint8)\n",
    "\n",
    "\n",
    "def _flatten_submission(mask: np.ndarray) -> np.ndarray:\n",
    "    return mask.astype(np.uint8).reshape(-1)\n",
    "\n",
    "\n",
    "def _write_submission(out_path: Path, sample_path: Path, predictions: Dict[str, np.ndarray]) -> str:\n",
    "    import pandas as pd\n",
    "\n",
    "    sample = pd.read_csv(sample_path)\n",
    "    if \"Id\" in sample.columns and \"Predicted\" in sample.columns:\n",
    "        concat = np.concatenate([_flatten_submission(predictions[k]) for k in sorted(predictions)])\n",
    "        n = min(len(sample), len(concat))\n",
    "        sample.loc[: n - 1, \"Predicted\"] = concat[:n]\n",
    "    elif {\"id\", \"predicted\"}.issubset(sample.columns):\n",
    "        concat = np.concatenate([_flatten_submission(predictions[k]) for k in sorted(predictions)])\n",
    "        n = min(len(sample), len(concat))\n",
    "        sample.loc[: n - 1, \"predicted\"] = concat[:n]\n",
    "    else:\n",
    "        concat = np.concatenate([_flatten_submission(predictions[k]) for k in sorted(predictions)])\n",
    "        sample = sample.iloc[: len(concat)].copy()\n",
    "        col = sample.columns[-1]\n",
    "        sample[col] = concat\n",
    "\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    sample.to_csv(out_path, index=False)\n",
    "    return str(out_path)\n",
    "\n",
    "\n",
    "def run_pipeline(cfg: NX46Config) -> Dict[str, object]:\n",
    "    root = Path(cfg.data_root)\n",
    "    nx46 = NX46AGNNVesuvius(cfg)\n",
    "\n",
    "    train_fragments = _discover_fragments(root, \"train\")\n",
    "    test_fragments = _discover_fragments(root, \"test\")\n",
    "\n",
    "    thresholds: List[float] = []\n",
    "    for idx, frag in enumerate(train_fragments, start=1):\n",
    "        nx46.logger.log_event(\n",
    "            f\"PROGRESS train={idx}/{len(train_fragments)} ({idx / max(len(train_fragments), 1) * 100:.1f}%)\"\n",
    "        )\n",
    "        volume_dir = frag / \"surface_volume\"\n",
    "        if not volume_dir.exists():\n",
    "            continue\n",
    "        stack = _read_stack_tif(volume_dir)\n",
    "        labels = _load_label_png(frag)\n",
    "        if labels is None:\n",
    "            continue\n",
    "        if labels.shape != stack.shape[1:]:\n",
    "            h = min(labels.shape[0], stack.shape[1])\n",
    "            w = min(labels.shape[1], stack.shape[2])\n",
    "            labels = labels[:h, :w]\n",
    "            stack = stack[:, :h, :w]\n",
    "        thresholds.append(nx46.train_threshold(stack, labels, frag.name))\n",
    "\n",
    "    threshold = float(np.median(np.array(thresholds, dtype=np.float32))) if thresholds else 0.5\n",
    "\n",
    "    predictions: Dict[str, np.ndarray] = {}\n",
    "    for idx, frag in enumerate(test_fragments, start=1):\n",
    "        nx46.logger.log_event(\n",
    "            f\"PROGRESS test={idx}/{len(test_fragments)} ({idx / max(len(test_fragments), 1) * 100:.1f}%)\"\n",
    "        )\n",
    "        volume_dir = frag / \"surface_volume\"\n",
    "        if not volume_dir.exists():\n",
    "            continue\n",
    "        stack = _read_stack_tif(volume_dir)\n",
    "        pred = nx46.infer_mask(stack, threshold, frag.name)\n",
    "        predictions[frag.name] = pred\n",
    "\n",
    "    sample_candidates = [\n",
    "        root / \"sample_submission.csv\",\n",
    "        Path(\"/kaggle/input/vesuvius-challenge-ink-detection/sample_submission.csv\"),\n",
    "    ]\n",
    "    sample_csv = next((p for p in sample_candidates if p.exists() and p.suffix == \".csv\"), None)\n",
    "    submission_path = None\n",
    "    if sample_csv and predictions:\n",
    "        submission_path = _write_submission(Path(cfg.work_root) / \"submission.csv\", sample_csv, predictions)\n",
    "\n",
    "    return nx46.finalize(\n",
    "        {\n",
    "            \"train_fragments\": [p.name for p in train_fragments],\n",
    "            \"test_fragments\": [p.name for p in test_fragments],\n",
    "            \"train_threshold\": threshold,\n",
    "            \"submission_path\": submission_path,\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = NX46Config(\n",
    "        data_root=os.environ.get(\"NX46_DATA_ROOT\", NX46Config.data_root),\n",
    "        work_root=os.environ.get(\"NX46_WORK_ROOT\", NX46Config.work_root),\n",
    "    )\n",
    "    result = run_pipeline(config)\n",
    "    print(json.dumps(result, indent=2, ensure_ascii=False))\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 15062069,
     "sourceId": 117682,
     "sourceType": "competition"
    },
    {
     "datasetId": 9431333,
     "sourceId": 14755914,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9462392,
     "sourceId": 14799025,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9470090,
     "sourceId": 14809675,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3.358669,
   "end_time": "2026-02-16T14:28:29.505112",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-02-16T14:28:26.146443",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
