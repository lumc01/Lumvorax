{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "779ca32c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-24T15:39:56.050758Z",
     "iopub.status.busy": "2026-02-24T15:39:56.050492Z",
     "iopub.status.idle": "2026-02-24T15:40:15.111231Z",
     "shell.execute_reply": "2026-02-24T15:40:15.110335Z"
    },
    "papermill": {
     "duration": 19.066185,
     "end_time": "2026-02-24T15:40:15.112891",
     "exception": false,
     "start_time": "2026-02-24T15:39:56.046706",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: /kaggle/input/nx47-dependencies\n",
      "Processing /kaggle/input/nx47-dependencies/imagecodecs-2026.1.14-cp311-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from imagecodecs) (2.0.2)\n",
      "Installing collected packages: imagecodecs\n",
      "Successfully installed imagecodecs-2026.1.14\n",
      "Looking in links: /kaggle/input/nx47-dependencies\n",
      "Requirement already satisfied: tifffile in /usr/local/lib/python3.12/dist-packages (2025.10.16)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from tifffile) (2.0.2)\n",
      "[1771947606157947139][SEQ:000001] LOGGER_INIT {\"gpu\": true}\n",
      "[1771947606183848702][SEQ:000001][SYSTEM][f3d09755a03040dc] LOGGER_V28_INIT | DATA: {\"session\": \"64eb880b8cb38282\"}\n",
      "[1771947606184935365][SEQ:000002] NX47_OK {}\n",
      "[1771947606199081286][SEQ:000003] AIMO3_OK {}\n",
      "[1771947606411710438][SEQ:000004] FILES_READY {\"count\": 1, \"run_tag\": \"v61.9-default\"}\n",
      "[1771947609278347309][SEQ:000002][INFO][40896d8735d34c5a] BLOC1_START: Performance brute CPU/Mémoire\n",
      "[1771947609279542714][SEQ:000003][INFO][4cf7bcf2956c69b2] BLOC1_TEST_CREATE_DESTROY: iterations=10000\n",
      "[1771947609295414297][SEQ:000004][METRIC][30b3ce6199222edf] METRIC: B1_CREATE_DESTROY_OPS_SEC=1272294.4150346522ops/s\n",
      "[1771947609296326322][SEQ:000005][METRIC][d679e15fd23cbdb8] METRIC: B1_CREATE_DESTROY_MEAN=785.9816ns\n",
      "[1771947609297548230][SEQ:000006][INFO][a5cdc8cb1a871eb8] BLOC1_TEST_MOVE: size=1024, iterations=5000\n",
      "[1771947609301236147][SEQ:000007][METRIC][57951d0f509931b6] METRIC: B1_MOVE_THROUGHPUT=3.5585852955363384GB/s\n",
      "[1771947609301831518][SEQ:000008][INFO][00fdc9cf6e4c42d0] BLOC1_TEST_FUSE: n_elements=100, iterations=1000\n",
      "[1771947609333125212][SEQ:000009][METRIC][65ec40cc10f6d193] METRIC: B1_FUSE_ELEM_SEC=9739760.419425251elem/s\n",
      "[1771947609333949494][SEQ:000010][INFO][ce2879cf5359de72] BLOC1_TEST_SPLIT: size=4096, splits=8, iterations=1000\n",
      "[1771947609337034768][SEQ:000011][METRIC][793c6579fe673ed1] METRIC: B1_SPLIT_PER_SEC=5338721.883287531splits/s\n",
      "[1771947609337870925][SEQ:000012][INFO][466622c8959e5ca1] BLOC1_TEST_ALIGN_AB: size=8192, iterations=2000\n",
      "[1771947609656222715][SEQ:000013][METRIC][0f1d530c569cd6f8] METRIC: B1_ALIGN_SPEEDUP=0.7067630850836963x\n",
      "[1771947609657307317][SEQ:000014][INFO][f6392536f5424e8b] BLOC1_TEST_SCALING: base_work=10000, threads=[1, 2, 4]\n",
      "[1771947609661492339][SEQ:000015][METRIC][78efad6313292df2] METRIC: B1_SCALING_2T_EFF=56.5636579058612%\n",
      "[1771947609662208705][SEQ:000016][INFO][2ae85a916ba95261] BLOC1_COMPLETE | DATA: {\"tests\": 6}\n",
      "[1771947609663083220 ns][AUDIT] SOLVER_INIT: sum all values\n",
      "[1771947609693209248][SEQ:000005] FUSION_SCORE_GLOBAL {\"file\": \"1407735.tif\", \"score\": 0.0}\n",
      "[1771947614799012090][SEQ:000006] FILE_DONE {\"file\": \"1407735.tif\", \"checksum\": \"1ffbed07500aba29\", \"slices\": 320}\n",
      "\n",
      "===== SLICE SUMMARY (ALL FILES) =====\n",
      "            latency_ms                                   fusion_score       \\\n",
      "                 count       mean       min          max        count mean   \n",
      "file                                                                         \n",
      "1407735.tif        320  13.879574  8.699257  1151.045354          320  0.0   \n",
      "\n",
      "                      weight            ...       p_hi             p_lo  \\\n",
      "             min  max  count      mean  ...        min        max count   \n",
      "file                                    ...                               \n",
      "1407735.tif  0.0  0.0    320  0.181828  ...  90.633904  92.407372   320   \n",
      "\n",
      "                                             w_mean                      \\\n",
      "                  mean        min        max  count      mean       min   \n",
      "file                                                                      \n",
      "1407735.tif  85.542331  84.633904  86.407372    320  0.109108  0.098496   \n",
      "\n",
      "                       \n",
      "                  max  \n",
      "file                   \n",
      "1407735.tif  0.120121  \n",
      "\n",
      "[1 rows x 24 columns]\n",
      "\n",
      "===== GLOBAL SLICE MEAN (ALL FILES) =====\n",
      "13.879574175000002 ms\n",
      "\n",
      "===== FILE CHECKSUMS =====\n",
      "       file         checksum  slices\n",
      "1407735.tif 1ffbed07500aba29     320\n",
      "[1771947615097845974][SEQ:000007] SUBMISSION_READY {\"zip\": \"/kaggle/working/submission.zip\"}\n",
      "[1771947615097905901][SEQ:000008] EXEC_COMPLETE {}\n",
      "[1771947615098224426][SEQ:000009] DEPENDENCY_MANIFEST_WRITTEN {\"paths\": [\"/kaggle/input/datasets/ndarray2000/nx47-dependencies\", \"/kaggle/input/nx47-dependencies\"], \"packages\": {\"imagecodecs\": true, \"tifffile\": true}}\n",
      "READY: /kaggle/working/submission.zip\n",
      "[1771947615108562230][SEQ:000010] GITHUB_CHUNK_EXPORT {\"chunk_count\": 1}\n"
     ]
    }
   ],
   "source": [
    "# Auto-generated from nx47-vesu-kernel-new-v61-4.ipynb\n",
    "# Source notebook is kept intact.\n",
    "\n",
    "# %% [code cell 1]\n",
    "# ============================================================\n",
    "# NX47-VESU KERNEL V61.9 ULTRA-AGGRESSIVE++ ULTRA-DEBUG++ — ONE-CELL ULTRA-FORNSIC (V28-COMPAT)\n",
    "# GPU STRICT • NON-LINEAR FUSION • DYNAMIC SLICE WEIGHT • ABLATION READY\n",
    "# FULL SLICE-LOCAL LOGGING: fusion_score, weight, p_hi, p_lo, w\n",
    "# ============================================================\n",
    "\n",
    "# ---------------------- INSTALL (OFFLINE SAFE) ----------------------\n",
    "import sys, subprocess, os\n",
    "from pathlib import Path\n",
    "NX47_DRY_RUN_EARLY = os.environ.get(\"NX47_DRY_RUN\", \"0\") == \"1\"\n",
    "def install_offline(package_name):\n",
    "    if NX47_DRY_RUN_EARLY:\n",
    "        return\n",
    "    wheel_roots = [\n",
    "        \"/kaggle/input/datasets/ndarray2000/nx47-dependencies\",\n",
    "        \"/kaggle/input/nx47-dependencies\",\n",
    "    ]\n",
    "    for root in wheel_roots:\n",
    "        if Path(root).exists():\n",
    "            try:\n",
    "                subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--no-index\", f\"--find-links={root}\", package_name])\n",
    "                return\n",
    "            except Exception:\n",
    "                continue\n",
    "    print(f\"Offline install failed for {package_name}, attempting standard install...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", package_name])\n",
    "\n",
    "install_offline(\"imagecodecs\")\n",
    "install_offline(\"tifffile\")\n",
    "\n",
    "# ---------------------- IMPORTS ----------------------\n",
    "import os, time, json, hashlib, gc, zipfile, threading, importlib.util, sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# GitHub TIFF chunk export utility (Kaggle notebook safe: __file__ may be undefined)\n",
    "_THIS_FILE = globals().get(\"__file__\", \"\")\n",
    "if _THIS_FILE:\n",
    "    try:\n",
    "        _REPO_ROOT = Path(_THIS_FILE).resolve().parents[2]\n",
    "    except Exception:\n",
    "        _REPO_ROOT = Path.cwd()\n",
    "else:\n",
    "    _REPO_ROOT = Path.cwd()\n",
    "\n",
    "for _cand in [\n",
    "    _REPO_ROOT / \"RAPPORT-VESUVIUS/export_utils\",\n",
    "    Path.cwd() / \"RAPPORT-VESUVIUS/export_utils\",\n",
    "    Path(\"/workspace/Lumvorax/RAPPORT-VESUVIUS/export_utils\"),\n",
    "]:\n",
    "    if _cand.exists():\n",
    "        sys.path.append(str(_cand))\n",
    "\n",
    "try:\n",
    "    from github_tiff_chunker import export_tiff_chunks, ChunkRule\n",
    "except Exception:\n",
    "    class ChunkRule:  # local fallback\n",
    "        def __init__(self, max_tiffs_per_chunk=90, max_chunk_size_bytes=18 * 1024 * 1024):\n",
    "            self.max_tiffs_per_chunk = int(max_tiffs_per_chunk)\n",
    "            self.max_chunk_size_bytes = int(max_chunk_size_bytes)\n",
    "\n",
    "    def export_tiff_chunks(input_root, output_root, rule=None):\n",
    "        rule = rule or ChunkRule()\n",
    "        src = Path(input_root)\n",
    "        out = Path(output_root)\n",
    "        out.mkdir(parents=True, exist_ok=True)\n",
    "        tiffs = sorted([p for p in src.rglob(\"*\") if p.is_file() and p.suffix.lower() in {\".tif\", \".tiff\"}])\n",
    "        chunks, i, cidx = [], 0, 1\n",
    "        while i < len(tiffs):\n",
    "            cdir = out / f\"chunk_{cidx:04d}\"\n",
    "            cdir.mkdir(parents=True, exist_ok=True)\n",
    "            count, size = 0, 0\n",
    "            while i < len(tiffs) and count < rule.max_tiffs_per_chunk:\n",
    "                p = tiffs[i]\n",
    "                ps = p.stat().st_size\n",
    "                if count > 0 and size + ps > rule.max_chunk_size_bytes:\n",
    "                    break\n",
    "                import shutil\n",
    "                shutil.copy2(p, cdir / p.name)\n",
    "                count += 1\n",
    "                size += ps\n",
    "                i += 1\n",
    "            chunks.append({\"chunk_name\": cdir.name, \"tiff_count\": count, \"total_size_bytes\": size})\n",
    "            cidx += 1\n",
    "        manifest = {\n",
    "            \"source\": str(src),\n",
    "            \"output\": str(out),\n",
    "            \"rules\": {\"max_tiffs_per_chunk\": rule.max_tiffs_per_chunk, \"max_chunk_size_bytes\": rule.max_chunk_size_bytes},\n",
    "            \"total_tiffs\": len(tiffs),\n",
    "            \"chunk_count\": len(chunks),\n",
    "            \"chunks\": chunks,\n",
    "        }\n",
    "        (out / \"chunk_manifest.json\").write_text(json.dumps(manifest, indent=2), encoding=\"utf-8\")\n",
    "        return manifest\n",
    "\n",
    "# ---------------------- GPU STRICT ----------------------\n",
    "GPU_STRICT = not NX47_DRY_RUN_EARLY\n",
    "try:\n",
    "    import cupy as cp\n",
    "    cp.cuda.runtime.getDeviceCount()\n",
    "    xp = cp\n",
    "    GPU = True\n",
    "except:\n",
    "    if GPU_STRICT:\n",
    "        raise RuntimeError(\"GPU REQUIRED — GPU_STRICT ENABLED\")\n",
    "    xp = np\n",
    "    GPU = False\n",
    "\n",
    "if GPU:\n",
    "    from cupyx.scipy.ndimage import gaussian_filter\n",
    "else:\n",
    "    from scipy.ndimage import gaussian_filter\n",
    "\n",
    "# ---------------------- PATHS ----------------------\n",
    "if NX47_DRY_RUN_EARLY:\n",
    "    ROOT = Path(os.environ.get(\"NX47_DRY_ROOT\", \"/tmp/nx47_dry\"))\n",
    "    TEST_DIR = ROOT / \"test_images\"\n",
    "    OUT = ROOT / \"tmp\"\n",
    "    OUT.mkdir(parents=True, exist_ok=True)\n",
    "    ZIP_PATH = ROOT / \"submission.zip\"\n",
    "else:\n",
    "    ROOT = Path(\"/kaggle/input/competitions/vesuvius-challenge-surface-detection\")\n",
    "    TEST_DIR = ROOT / \"test_images\"\n",
    "    OUT = Path(\"/kaggle/working/tmp\")\n",
    "    OUT.mkdir(exist_ok=True)\n",
    "    ZIP_PATH = Path(\"/kaggle/working/submission.zip\")\n",
    "\n",
    "# ---------------------- LOGGER ULTRA-DEBUG++ ----------------------\n",
    "class Logger:\n",
    "    def __init__(self):\n",
    "        self.seq = 0\n",
    "        self.slice_rows = []\n",
    "        self.file_rows = []\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def _safe(self, v):\n",
    "        if hasattr(v, \"item\"): return v.item()\n",
    "        if isinstance(v, np.ndarray): return float(np.mean(v))\n",
    "        return v\n",
    "\n",
    "    def log(self, msg, data=None):\n",
    "        with self.lock:\n",
    "            self.seq += 1\n",
    "            payload = {k:self._safe(v) for k,v in (data or {}).items()}\n",
    "            print(f\"[{time.time_ns()}][SEQ:{self.seq:06d}] {msg} {json.dumps(payload)}\")\n",
    "\n",
    "    def slice_metric(self, file, z, latency_ns, fusion_score=None, weight=None, p_hi=None, p_lo=None, w=None):\n",
    "        self.slice_rows.append({\n",
    "            \"file\": file,\n",
    "            \"slice\": z,\n",
    "            \"latency_ms\": latency_ns / 1e6,\n",
    "            \"fusion_score\": fusion_score,\n",
    "            \"weight\": weight,\n",
    "            \"p_hi\": p_hi,\n",
    "            \"p_lo\": p_lo,\n",
    "            \"w_mean\": w if w is None else float(np.mean(w))\n",
    "        })\n",
    "\n",
    "    def file_metric(self, file, checksum, slices):\n",
    "        self.file_rows.append({\n",
    "            \"file\": file,\n",
    "            \"checksum\": checksum,\n",
    "            \"slices\": slices\n",
    "        })\n",
    "\n",
    "logger = Logger()\n",
    "logger.log(\"LOGGER_INIT\", {\"gpu\": GPU})\n",
    "\n",
    "# ---------------------- NX47 ----------------------\n",
    "if NX47_DRY_RUN_EARLY:\n",
    "    class _DummyNX47:\n",
    "        def run_all(self):\n",
    "            return {}\n",
    "    nx47 = _DummyNX47()\n",
    "    solve_aimo3 = lambda _q: {}\n",
    "    logger.log(\"NX47_AIMO3_DRYRUN_BYPASS\")\n",
    "else:\n",
    "    sys.path.append(\"/kaggle/input/datasets/ndarray2000/nx47-arc-kernel-v2-fixed-py\")\n",
    "    from nx47_arc_kernel_v2_fixed import PerformanceProofBloc1\n",
    "    nx47 = PerformanceProofBloc1()\n",
    "    logger.log(\"NX47_OK\")\n",
    "\n",
    "    # ---------------------- AIMO3 ----------------------\n",
    "    aimo3_path = \"/kaggle/input/datasets/ndarray2000/iamo3-shf-resonance-v3/aimo3_shf_resonance_v3.py\"\n",
    "    spec = importlib.util.spec_from_file_location(\"aimo3\", aimo3_path)\n",
    "    aimo3 = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(aimo3)\n",
    "    solve_aimo3 = aimo3.solve_enhanced\n",
    "    logger.log(\"AIMO3_OK\")\n",
    "\n",
    "# ---------------------- TIFF ----------------------\n",
    "import tifffile\n",
    "def read_tiff(p):\n",
    "    return tifffile.imread(p).astype(np.float32)\n",
    "\n",
    "# ---------------------- RUN CONFIG ----------------------\n",
    "\n",
    "RUN_TAG = os.environ.get(\"NX47_RUN_TAG\", \"v61.9-default\")\n",
    "DRY_RUN = os.environ.get(\"NX47_DRY_RUN\", \"0\") == \"1\"\n",
    "PCTL_HI_BASE = float(os.environ.get(\"NX47_PCTL_HI_BASE\", \"88\"))\n",
    "PCTL_HI_GAIN = float(os.environ.get(\"NX47_PCTL_HI_GAIN\", \"6\"))\n",
    "PCTL_HI_MIN = float(os.environ.get(\"NX47_PCTL_HI_MIN\", \"88\"))\n",
    "PCTL_HI_MAX = float(os.environ.get(\"NX47_PCTL_HI_MAX\", \"96\"))\n",
    "PCTL_LO_GAP = float(os.environ.get(\"NX47_PCTL_LO_GAP\", \"6\"))\n",
    "PCTL_LO_MIN = float(os.environ.get(\"NX47_PCTL_LO_MIN\", \"80\"))\n",
    "\n",
    "# ---------------------- AUTO THRESHOLD SLICE-LOCAL ----------------------\n",
    "def slice_percentiles(slice_data):\n",
    "    flat = slice_data.ravel()\n",
    "    p90 = np.percentile(flat, 90)\n",
    "    p95 = np.percentile(flat, 95)\n",
    "    std = np.std(flat)\n",
    "    hi = float(np.clip(PCTL_HI_BASE + PCTL_HI_GAIN*(p95 - p90)/(std + 1e-6), PCTL_HI_MIN, PCTL_HI_MAX))\n",
    "    lo = max(hi - PCTL_LO_GAP, PCTL_LO_MIN)\n",
    "    return hi, lo\n",
    "\n",
    "# ---------------------- PROCESS ONE FILE ----------------------\n",
    "def process_file(path, ablation=None, ultra_aggressive=True):\n",
    "    t_file = time.time_ns()\n",
    "\n",
    "    vol = read_tiff(path)\n",
    "    vol = (vol - vol.min()) / (vol.max() - vol.min() + 1e-6)\n",
    "    vol_gpu = xp.asarray(vol)\n",
    "\n",
    "    sigma = float(xp.std(vol_gpu) * 0.9 + 0.4)\n",
    "    smooth = gaussian_filter(vol_gpu, sigma=sigma)\n",
    "    resid = vol_gpu - smooth\n",
    "\n",
    "    # ----- SCORES (REAL, DYNAMIC) -----\n",
    "    nx_vals = [v for v in nx47.run_all().values() if isinstance(v,(int,float))]\n",
    "    nx_score = float(np.mean(nx_vals)) if nx_vals else 0.0\n",
    "    a3 = solve_aimo3(\"sum all values\")\n",
    "    a3_score = float(np.mean(list(a3.values()))) if isinstance(a3,dict) and a3 else 0.0\n",
    "\n",
    "    # ----- ABLATION CONTROL -----\n",
    "    if ablation == \"nx47\": a3_score = 0.0\n",
    "    if ablation == \"aimo3\": nx_score = 0.0\n",
    "\n",
    "    # ----- NON-LINEAR FUSION -----\n",
    "    fusion_score_global = 0.7*np.tanh(nx_score*2.0) + 0.3*np.tanh(a3_score*2.5)\n",
    "\n",
    "    # ----- ULTRA-AGGRESSIVE ADAPTIVE BOOST -----\n",
    "    if ultra_aggressive:\n",
    "        vol_std_global = float(xp.std(vol_gpu))\n",
    "        fusion_score_global *= 1.0 + 0.5*np.tanh(vol_std_global*1.5)\n",
    "\n",
    "    logger.log(\"FUSION_SCORE_GLOBAL\", {\"file\": path.name, \"score\": fusion_score_global})\n",
    "\n",
    "    Z = vol_gpu.shape[0]\n",
    "    out = []\n",
    "\n",
    "    for z in range(Z):\n",
    "        t0 = time.time_ns()\n",
    "        z0, z1 = max(0, z-1), min(Z, z+2)\n",
    "        proj = xp.mean(resid[z0:z1], axis=0)\n",
    "\n",
    "        # ----- LOCAL MULTI-SCALE VARIANCE -----\n",
    "        local_vol = vol_gpu[max(0,z-2):min(Z,z+3)]\n",
    "        local_std = float(xp.std(local_vol))\n",
    "\n",
    "        # ----- ULTRA-AGGRESSIVE WEIGHT DYNAMIC (FUSION SCORE IMPACT) -----\n",
    "        weight_base = 0.15\n",
    "        if ultra_aggressive:\n",
    "            weight = weight_base + 0.25*np.tanh(fusion_score_global)*np.tanh(local_std*2.0) \\\n",
    "                     + 0.1*np.tanh(local_std*3.0)\n",
    "        else:\n",
    "            weight = weight_base + 0.25*np.tanh(fusion_score_global)*np.tanh(local_std*2.0)\n",
    "        proj = proj + weight\n",
    "\n",
    "        # ----- SLICE-LOCAL AUTO THRESHOLD -----\n",
    "        proj_cpu = xp.asnumpy(proj)\n",
    "        p_hi, p_lo = slice_percentiles(proj_cpu)\n",
    "\n",
    "        mask_hi = proj > np.percentile(proj_cpu, p_hi)\n",
    "        mask_lo = proj > np.percentile(proj_cpu, p_lo)\n",
    "        w = xp.clip((proj - np.percentile(proj_cpu, p_lo)) /\n",
    "                    (np.percentile(proj_cpu, p_hi) - np.percentile(proj_cpu, p_lo) + 1e-6), 0.0, 1.0)\n",
    "        final = (w * xp.logical_and(mask_hi, mask_lo) +\n",
    "                 (1.0 - w) * xp.logical_or(mask_hi, mask_lo)) > 0.5\n",
    "        out.append(final.astype(xp.uint8))\n",
    "\n",
    "        latency = time.time_ns() - t0\n",
    "        # ----- ULTRA-DEBUG++ LOGGING -----\n",
    "        logger.slice_metric(\n",
    "            path.name, z, latency,\n",
    "            fusion_score=float(fusion_score_global),\n",
    "            weight=float(weight),\n",
    "            p_hi=p_hi,\n",
    "            p_lo=p_lo,\n",
    "            w=w\n",
    "        )\n",
    "\n",
    "    mask = xp.stack(out)\n",
    "    checksum = hashlib.sha256(vol.tobytes()).hexdigest()[:16]\n",
    "\n",
    "    logger.file_metric(path.name, checksum, Z)\n",
    "    logger.log(\"FILE_DONE\", {\"file\": path.name, \"checksum\": checksum, \"slices\": Z})\n",
    "\n",
    "        # NX47 v61.2: align to competitor-like binary uint8 domain (0/1) while preserving 3D multipage TIFF.\n",
    "    return xp.asnumpy(mask).astype(np.uint8)\n",
    "\n",
    "# ---------------------- RUN ALL ----------------------\n",
    "if DRY_RUN:\n",
    "    logger.log(\"DRY_RUN_EXIT\", {\"run_tag\": RUN_TAG, \"note\": \"preflight mode before Kaggle IO scan\"})\n",
    "    print(\"DRY_RUN READY:\", ZIP_PATH)\n",
    "    raise SystemExit(0)\n",
    "\n",
    "FILES = sorted(TEST_DIR.rglob(\"*.tif\"))\n",
    "if not FILES:\n",
    "    raise RuntimeError(\"NO TEST FILES FOUND\")\n",
    "\n",
    "logger.log(\"FILES_READY\", {\"count\": len(FILES), \"run_tag\": RUN_TAG})\n",
    "\n",
    "\n",
    "with zipfile.ZipFile(ZIP_PATH, \"w\", zipfile.ZIP_STORED) as zf:\n",
    "    for f in FILES:\n",
    "        m = process_file(f)  # ablation=None, ultra_aggressive=True\n",
    "        out = OUT / f.name\n",
    "        if m.ndim != 3:\n",
    "            raise RuntimeError(f\"Invalid NX47 output rank for {f.name}: {m.shape} (expected 3D Z,H,W)\")\n",
    "        binary_mode = os.environ.get(\"NX47_BINARY_MODE\", \"0_1\").strip().lower()\n",
    "        if binary_mode not in {\"0_1\", \"0_255\"}:\n",
    "            raise RuntimeError(f\"Invalid NX47_BINARY_MODE: {binary_mode}\")\n",
    "        m_write = m.astype(np.uint8)\n",
    "        if binary_mode == \"0_255\":\n",
    "            m_write = (m_write > 0).astype(np.uint8) * 255\n",
    "        else:\n",
    "            m_write = (m_write > 0).astype(np.uint8)\n",
    "        tifffile.imwrite(out, m_write, compression=\"LZW\")\n",
    "        zf.write(out, arcname=f.name)\n",
    "        out.unlink()\n",
    "        gc.collect()\n",
    "\n",
    "# ---------------------- GLOBAL SUMMARY ----------------------\n",
    "df_slices = pd.DataFrame(logger.slice_rows)\n",
    "df_files  = pd.DataFrame(logger.file_rows)\n",
    "\n",
    "print(\"\\n===== SLICE SUMMARY (ALL FILES) =====\")\n",
    "print(df_slices.groupby(\"file\")[[\"latency_ms\",\"fusion_score\",\"weight\",\"p_hi\",\"p_lo\",\"w_mean\"]].agg([\"count\",\"mean\",\"min\",\"max\"]))\n",
    "\n",
    "print(\"\\n===== GLOBAL SLICE MEAN (ALL FILES) =====\")\n",
    "print(df_slices[\"latency_ms\"].mean(), \"ms\")\n",
    "\n",
    "print(\"\\n===== FILE CHECKSUMS =====\")\n",
    "print(df_files.to_string(index=False))\n",
    "\n",
    "logger.log(\"SUBMISSION_READY\", {\"zip\": str(ZIP_PATH)})\n",
    "logger.log(\"EXEC_COMPLETE\")\n",
    "\n",
    "\n",
    "# ---- V61.9 forensic360 add-on ----\n",
    "try:\n",
    "    dep_manifest = {\n",
    "        \"paths\": [\"/kaggle/input/datasets/ndarray2000/nx47-dependencies\", \"/kaggle/input/nx47-dependencies\"],\n",
    "        \"packages\": {\n",
    "            \"imagecodecs\": importlib.util.find_spec(\"imagecodecs\") is not None,\n",
    "            \"tifffile\": importlib.util.find_spec(\"tifffile\") is not None,\n",
    "        },\n",
    "    }\n",
    "    Path('/kaggle/working/dependency_manifest_v619.json').write_text(json.dumps(dep_manifest, indent=2), encoding='utf-8')\n",
    "    logger.log(\"DEPENDENCY_MANIFEST_WRITTEN\", dep_manifest)\n",
    "except Exception as _dep_exc:\n",
    "    logger.log(\"DEPENDENCY_MANIFEST_FAILED\", {\"error\": str(_dep_exc)})\n",
    "\n",
    "for alias in [Path(\"/kaggle/working/nx47_vesuvius/submission.zip\"), Path(\"submission.zip\"), Path(\"nx47_vesuvius/submission.zip\")]:\n",
    "    alias.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if alias.resolve() != ZIP_PATH.resolve():\n",
    "        import shutil\n",
    "        shutil.copyfile(ZIP_PATH, alias)\n",
    "\n",
    "print(\"READY:\", ZIP_PATH)\n",
    "\n",
    "\n",
    "\n",
    "# ---- V61.9 GitHub export add-on ----\n",
    "try:\n",
    "    extracted = Path('/kaggle/working/submission_v619_extracted')\n",
    "    extracted.mkdir(parents=True, exist_ok=True)\n",
    "    if ZIP_PATH.exists():\n",
    "        with zipfile.ZipFile(ZIP_PATH) as _zf:\n",
    "            _zf.extractall(extracted)\n",
    "        chunk_manifest = export_tiff_chunks(\n",
    "            extracted,\n",
    "            Path('/kaggle/working/github_tiff_chunks_v619'),\n",
    "            ChunkRule(max_tiffs_per_chunk=90, max_chunk_size_bytes=18 * 1024 * 1024),\n",
    "        )\n",
    "        Path('/kaggle/working/export_manifest_v619.json').write_text(json.dumps(chunk_manifest, indent=2), encoding='utf-8')\n",
    "        logger.log(\"GITHUB_CHUNK_EXPORT\", {\"chunk_count\": chunk_manifest.get(\"chunk_count\", 0)})\n",
    "    else:\n",
    "        Path('/kaggle/working/export_manifest_v619.json').write_text(json.dumps({\"warning\":\"submission.zip missing\"}, indent=2), encoding='utf-8')\n",
    "except Exception as _chunk_exc:\n",
    "    logger.log(\"GITHUB_CHUNK_EXPORT_FAILED\", {\"error\": str(_chunk_exc)})\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 15062069,
     "sourceId": 117682,
     "sourceType": "competition"
    },
    {
     "databundleVersionId": 15653904,
     "datasetId": 9462392,
     "sourceId": 14799025,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 15606459,
     "datasetId": 9431333,
     "sourceId": 14755914,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 15665516,
     "datasetId": 9470090,
     "sourceId": 14809675,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 21.924558,
   "end_time": "2026-02-24T15:40:15.532436",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-02-24T15:39:53.607878",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
